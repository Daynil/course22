{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Fastai\n",
    "\n",
    "To make sense of the NLP chapter, I'm building out the imdb classifier using all 3 libraries, fastai, hugging faces, and pytorch. It doesn't seem like it makes sense to do a pure python one yet since we did not go over embeddings very much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai.text.all as fai_text\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/daynil/.fastai/data/imdb')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = fai_text.untar_data(fai_text.URLs.IMDB)\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = fai_text.get_text_files(path, folders=['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What could have been an excellent hostage movie was totally ruined by what '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = files[0].open().read()\n",
    "txt[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = fai_text.L(o.open().read() for o in files[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and concat the corpus of text, creating a tmp directory with the corpus in a temporary directory (default `./tmp`). \n",
    "\n",
    "Finds the common sequences of characters to create a vocab. E.g., most frequently occuring sequences of chars get their own token.\n",
    "\n",
    "Fastai uses the google tokenizer library [sentencepiece](https://github.com/google/sentencepiece) to do this.\n",
    "\n",
    "After tokenization, the corpus file is deleted and a tokenizer model and vocab file are created in the temporary directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'sp_model': Path('tmp/spm.model')}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = fai_text.SubwordTokenizer()\n",
    "sp.setup(txts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁What ▁could ▁have ▁been ▁an ▁excellent ▁hostage ▁movie ▁was ▁totally ▁ruin'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = sp([txt])\n",
    "\" \".join(next(toks))[:75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastai adds its own functionality on top of google's subword tokenizer. It adds special tokens, like xxbos (beginning of stream indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#234) ['▁xxbos','▁xxmaj','▁what','▁could','▁have','▁been','▁an','▁excellent','▁hostage','▁movie','▁was','▁totally','▁ruined','▁by','▁what','▁apparently','▁looks','▁like','▁a','▁bored','▁director','▁...','▁there','▁were','▁so','▁many','▁direction','s','▁that','▁the','▁movie'...]\n"
     ]
    }
   ],
   "source": [
    "tkn = fai_text.Tokenizer(sp)\n",
    "# Note coll_repr is literally just printing the first x items of a list\n",
    "# But makes it easier to work with lists that are possibly generators, so we'll use that\n",
    "print(fai_text.coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to numericalize our tokens, which just means replacing each token with its index in the vocab.\n",
    "\n",
    "We'll use a small sample of 200 instead of the full corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁xxbos', '▁xxmaj', '▁what', '▁could']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks200 = txts[:200].map(tkn)\n",
    "toks200[0][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(#2464) [\\'xxunk\\',\\'xxpad\\',\\'xxbos\\',\\'xxeos\\',\\'xxfld\\',\\'xxrep\\',\\'xxwrep\\',\\'xxup\\',\\'xxmaj\\',\\'▁xxmaj\\',\\'▁the\\',\\'.\\',\\',\\',\\'s\\',\\'▁a\\',\\'▁of\\',\\'▁and\\',\\'▁to\\',\"\\'\",\\'▁it\\'...]'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = fai_text.Numericalize()\n",
    "num.setup(toks200)\n",
    "fai_text.coll_repr(num.vocab, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#234) ['▁xxbos','▁xxmaj','▁what','▁could','▁have','▁been','▁an','▁excellent','▁hostage','▁movie','▁was','▁totally','▁ruined','▁by','▁what','▁apparently','▁looks','▁like','▁a','▁bored'...]\""
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = tkn(txt)\n",
    "fai_text.coll_repr(toks, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(#234) [TensorText(51),TensorText(9),TensorText(72),TensorText(115),TensorText(44),TensorText(103),TensorText(58),TensorText(700),TensorText(1280),TensorText(28),TensorText(27),TensorText(644),TensorText(0),TensorText(54),TensorText(72),TensorText(1088),TensorText(534),TensorText(55),TensorText(14),TensorText(1881)...]'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = num(toks)\n",
    "fai_text.coll_repr(nums, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁what ▁could ▁have\n"
     ]
    }
   ],
   "source": [
    "print(num.vocab[72], num.vocab[115], num.vocab[44])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to set up a way of feeding a large corpus of text into a language model to train it. \n",
    "\n",
    "With images, we had to resize each image so that it was a consistent size, e.g. 224x224px. This is because tensors require a regular shape in order to function. However, we cannot simply resize text to whatever length we want.\n",
    "\n",
    "Training a language model involves (in this case) asking it to predict the *next word* in some text. Importantly, that means *order matters*. \n",
    "\n",
    "What we can do is concat the entire corpus into a single text stream, then break it out into a number of batches, where each batch starts where the last one ended.\n",
    "\n",
    "Using this text as an example:\n",
    "> In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = \"In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\"\n",
    "tokens = tkn(stream)\n",
    "bs, seq_len = 6, 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>▁xxbos</td>\n",
       "      <td>▁xxmaj</td>\n",
       "      <td>▁in</td>\n",
       "      <td>▁this</td>\n",
       "      <td>▁chapter</td>\n",
       "      <td>,</td>\n",
       "      <td>▁we</td>\n",
       "      <td>▁will</td>\n",
       "      <td>▁go</td>\n",
       "      <td>▁back</td>\n",
       "      <td>▁over</td>\n",
       "      <td>▁the</td>\n",
       "      <td>▁example</td>\n",
       "      <td>▁of</td>\n",
       "      <td>▁class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ifying</td>\n",
       "      <td>▁movie</td>\n",
       "      <td>▁reviews</td>\n",
       "      <td>▁we</td>\n",
       "      <td>▁studi</td>\n",
       "      <td>ed</td>\n",
       "      <td>▁in</td>\n",
       "      <td>▁chapter</td>\n",
       "      <td>▁1</td>\n",
       "      <td>▁and</td>\n",
       "      <td>▁dig</td>\n",
       "      <td>▁deep</td>\n",
       "      <td>er</td>\n",
       "      <td>▁under</td>\n",
       "      <td>▁the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>▁surface</td>\n",
       "      <td>.</td>\n",
       "      <td>▁xxmaj</td>\n",
       "      <td>▁first</td>\n",
       "      <td>▁we</td>\n",
       "      <td>▁will</td>\n",
       "      <td>▁look</td>\n",
       "      <td>▁at</td>\n",
       "      <td>▁the</td>\n",
       "      <td>▁process</td>\n",
       "      <td>ing</td>\n",
       "      <td>▁steps</td>\n",
       "      <td>▁necessary</td>\n",
       "      <td>▁to</td>\n",
       "      <td>▁convert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>▁text</td>\n",
       "      <td>▁into</td>\n",
       "      <td>▁numbers</td>\n",
       "      <td>▁and</td>\n",
       "      <td>▁how</td>\n",
       "      <td>▁to</td>\n",
       "      <td>▁custom</td>\n",
       "      <td>ize</td>\n",
       "      <td>▁it</td>\n",
       "      <td>.</td>\n",
       "      <td>▁xxmaj</td>\n",
       "      <td>▁by</td>\n",
       "      <td>▁doing</td>\n",
       "      <td>▁this</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>▁we</td>\n",
       "      <td>'</td>\n",
       "      <td>ll</td>\n",
       "      <td>▁have</td>\n",
       "      <td>▁another</td>\n",
       "      <td>▁example</td>\n",
       "      <td>▁of</td>\n",
       "      <td>▁the</td>\n",
       "      <td>▁pre</td>\n",
       "      <td>pro</td>\n",
       "      <td>ce</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>or</td>\n",
       "      <td>▁used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>▁in</td>\n",
       "      <td>▁the</td>\n",
       "      <td>▁da</td>\n",
       "      <td>ta</td>\n",
       "      <td>▁block</td>\n",
       "      <td>▁xxup</td>\n",
       "      <td>▁a</td>\n",
       "      <td>p</td>\n",
       "      <td>i</td>\n",
       "      <td>.</td>\n",
       "      <td>▁xxmaj</td>\n",
       "      <td>▁then</td>\n",
       "      <td>▁we</td>\n",
       "      <td>▁will</td>\n",
       "      <td>▁study</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1         2       3         4         5        6         7   \\\n",
       "0    ▁xxbos  ▁xxmaj       ▁in   ▁this  ▁chapter         ,      ▁we     ▁will   \n",
       "1    ifying  ▁movie  ▁reviews     ▁we    ▁studi        ed      ▁in  ▁chapter   \n",
       "2  ▁surface       .    ▁xxmaj  ▁first       ▁we     ▁will    ▁look       ▁at   \n",
       "3     ▁text   ▁into  ▁numbers    ▁and      ▁how       ▁to  ▁custom       ize   \n",
       "4       ▁we       '        ll   ▁have  ▁another  ▁example      ▁of      ▁the   \n",
       "5       ▁in    ▁the       ▁da      ta    ▁block     ▁xxup       ▁a         p   \n",
       "\n",
       "     8         9       10      11          12      13        14  \n",
       "0   ▁go     ▁back   ▁over    ▁the    ▁example     ▁of    ▁class  \n",
       "1    ▁1      ▁and    ▁dig   ▁deep          er  ▁under      ▁the  \n",
       "2  ▁the  ▁process     ing  ▁steps  ▁necessary     ▁to  ▁convert  \n",
       "3   ▁it         .  ▁xxmaj     ▁by      ▁doing   ▁this         ,  \n",
       "4  ▁pre       pro      ce       s           s      or     ▁used  \n",
       "5     i         .  ▁xxmaj   ▁then         ▁we   ▁will    ▁study  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.array([tokens[i*seq_len : (i+1)*seq_len] for i in range(bs)]))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 6 batches of streams **where the order is preserved**, we have the data in the format we need to be able to feed it into a model.\n",
    "\n",
    "However, one further wrinkle is that for a realistic corpus like IMDB reviews, this would be millions of columns wide, not just 15, even if we had a much larger batch size like 64.\n",
    "\n",
    "To solve this, we can create a left-to-right sliding window of mini-streams of data. This still **preserves the order**, but allows us to more tightly control the size of each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch of text\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>▁xxbos</td>\n",
       "      <td>▁xxmaj</td>\n",
       "      <td>▁in</td>\n",
       "      <td>▁this</td>\n",
       "      <td>▁chapter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ifying</td>\n",
       "      <td>▁movie</td>\n",
       "      <td>▁reviews</td>\n",
       "      <td>▁we</td>\n",
       "      <td>▁studi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>▁surface</td>\n",
       "      <td>.</td>\n",
       "      <td>▁xxmaj</td>\n",
       "      <td>▁first</td>\n",
       "      <td>▁we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>▁text</td>\n",
       "      <td>▁into</td>\n",
       "      <td>▁numbers</td>\n",
       "      <td>▁and</td>\n",
       "      <td>▁how</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>▁we</td>\n",
       "      <td>'</td>\n",
       "      <td>ll</td>\n",
       "      <td>▁have</td>\n",
       "      <td>▁another</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>▁in</td>\n",
       "      <td>▁the</td>\n",
       "      <td>▁da</td>\n",
       "      <td>ta</td>\n",
       "      <td>▁block</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0       1         2       3         4\n",
       "0    ▁xxbos  ▁xxmaj       ▁in   ▁this  ▁chapter\n",
       "1    ifying  ▁movie  ▁reviews     ▁we    ▁studi\n",
       "2  ▁surface       .    ▁xxmaj  ▁first       ▁we\n",
       "3     ▁text   ▁into  ▁numbers    ▁and      ▁how\n",
       "4       ▁we       '        ll   ▁have  ▁another\n",
       "5       ▁in    ▁the       ▁da      ta    ▁block"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs, seq_len = 6, 5\n",
    "df = pd.DataFrame(np.array([tokens[i*15 : i*15+seq_len] for i in range(bs)]))\n",
    "print(\"First batch of text\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second batch of text\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>,</td>\n",
       "      <td>▁we</td>\n",
       "      <td>▁will</td>\n",
       "      <td>▁go</td>\n",
       "      <td>▁back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ed</td>\n",
       "      <td>▁in</td>\n",
       "      <td>▁chapter</td>\n",
       "      <td>▁1</td>\n",
       "      <td>▁and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>▁will</td>\n",
       "      <td>▁look</td>\n",
       "      <td>▁at</td>\n",
       "      <td>▁the</td>\n",
       "      <td>▁process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>▁to</td>\n",
       "      <td>▁custom</td>\n",
       "      <td>ize</td>\n",
       "      <td>▁it</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>▁example</td>\n",
       "      <td>▁of</td>\n",
       "      <td>▁the</td>\n",
       "      <td>▁pre</td>\n",
       "      <td>pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>▁xxup</td>\n",
       "      <td>▁a</td>\n",
       "      <td>p</td>\n",
       "      <td>i</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0        1         2     3         4\n",
       "0         ,      ▁we     ▁will   ▁go     ▁back\n",
       "1        ed      ▁in  ▁chapter    ▁1      ▁and\n",
       "2     ▁will    ▁look       ▁at  ▁the  ▁process\n",
       "3       ▁to  ▁custom       ize   ▁it         .\n",
       "4  ▁example      ▁of      ▁the  ▁pre       pro\n",
       "5     ▁xxup       ▁a         p     i         ."
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs, seq_len = 6, 5\n",
    "df = pd.DataFrame(np.array([tokens[i*15+seq_len : i*15+2*seq_len] for i in range(bs)]))\n",
    "print(\"Second batch of text\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Third batch of text\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>▁over</td>\n",
       "      <td>▁the</td>\n",
       "      <td>▁example</td>\n",
       "      <td>▁of</td>\n",
       "      <td>▁class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>▁dig</td>\n",
       "      <td>▁deep</td>\n",
       "      <td>er</td>\n",
       "      <td>▁under</td>\n",
       "      <td>▁the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ing</td>\n",
       "      <td>▁steps</td>\n",
       "      <td>▁necessary</td>\n",
       "      <td>▁to</td>\n",
       "      <td>▁convert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>▁xxmaj</td>\n",
       "      <td>▁by</td>\n",
       "      <td>▁doing</td>\n",
       "      <td>▁this</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ce</td>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>or</td>\n",
       "      <td>▁used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>▁xxmaj</td>\n",
       "      <td>▁then</td>\n",
       "      <td>▁we</td>\n",
       "      <td>▁will</td>\n",
       "      <td>▁study</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1           2       3         4\n",
       "0   ▁over    ▁the    ▁example     ▁of    ▁class\n",
       "1    ▁dig   ▁deep          er  ▁under      ▁the\n",
       "2     ing  ▁steps  ▁necessary     ▁to  ▁convert\n",
       "3  ▁xxmaj     ▁by      ▁doing   ▁this         ,\n",
       "4      ce       s           s      or     ▁used\n",
       "5  ▁xxmaj   ▁then         ▁we   ▁will    ▁study"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs, seq_len = 6, 5\n",
    "df = pd.DataFrame(np.array([tokens[i*15+2*seq_len : i*15+3*seq_len] for i in range(bs)]))\n",
    "print(\"Third batch of text\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying this process to the IMDB reviews dataset, we can create a stream by combining the individual documents (each document is a text file with a single review).\n",
    "\n",
    "For more effecient training, we can randomize the order in which the documents are combined into a stream on each epoch. **Importantly, we randomize the order of the documents, not the order of the text within them**.\n",
    "\n",
    "Once we have a stream each epoch, we cut that stream into a batch of fixed-size *consecutive* mini-streams. The model then reads the mini-streams in order.\n",
    "\n",
    "This is done behind the scenes by the fastai `LMDataLoader`. Here, it picks a batch size of 64 automatically, and our stream length is 72."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 72]), torch.Size([64, 72]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums200 = toks200.map(num)\n",
    "dl = fai_text.LMDataLoader(nums200)\n",
    "x,y = fai_text.first(dl)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁xxbos ▁xxmaj ▁what ▁could ▁have ▁been ▁an ▁excellent ▁hostage ▁movie\n",
      "▁xxmaj ▁what ▁could ▁have ▁been ▁an ▁excellent ▁hostage ▁movie ▁was\n"
     ]
    }
   ],
   "source": [
    "# The independent variable is just the start of the text\n",
    "print(' '.join(num.vocab[o] for o in x[0][:10]))\n",
    "# And the label is the same thing, but offset by 1 token\n",
    "# In other words, we want our model to guess the next token, in this case \"_was\"\n",
    "print(' '.join(num.vocab[o] for o in y[0][:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen the individual pieces, we can create a dataloader for the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_imdb = partial(fai_text.get_text_files, folders=['train', 'test', 'unsup'])\n",
    "\n",
    "dls_lm = fai_text.DataBlock(\n",
    "    blocks=fai_text.TextBlock.from_folder(path, is_lm=True),\n",
    "    get_items=get_imdb,\n",
    "    splitter=fai_text.RandomSplitter(0.1)\n",
    ").dataloaders(path, path=path, bs=128, seq_len=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj kurt xxmaj russell is so believable and the action so non - stop that it takes thinking about it afterward to realize that there were honest - to - goodness important themes [ overcoming fear of xxmaj the xxmaj stranger , learning to rise above early conditioning , the strength that love and friendship can bring , etc . ] in the storyline . xxmaj this is so very rare for a ' guy 's action flick '</td>\n",
       "      <td>xxmaj kurt xxmaj russell is so believable and the action so non - stop that it takes thinking about it afterward to realize that there were honest - to - goodness important themes [ overcoming fear of xxmaj the xxmaj stranger , learning to rise above early conditioning , the strength that love and friendship can bring , etc . ] in the storyline . xxmaj this is so very rare for a ' guy 's action flick ' that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content to preside over a sham of a hearing ! xxmaj this is just the sort of film they should show young law students in order to elicit a few laughs at all the histrionics . \\n\\n xxmaj believe me that there are thousands of better films out there from the 1930s waiting to be discovered . xxmaj try almost xxup any film of the era and you 're bound to be better off than with this silly dud .</td>\n",
       "      <td>to preside over a sham of a hearing ! xxmaj this is just the sort of film they should show young law students in order to elicit a few laughs at all the histrionics . \\n\\n xxmaj believe me that there are thousands of better films out there from the 1930s waiting to be discovered . xxmaj try almost xxup any film of the era and you 're bound to be better off than with this silly dud . xxmaj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_lm.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He gets pretty handwavy at this point, ultimately saying we'll learn to build this model (a recurrent neural network architecture called AWD-LSTM) from scratch in a later chapter, so I will avoid spending too long diving into details here.\n",
    "\n",
    "Ultimately, this part of the process also converts the integer word indices into activations by using embeddings, which is what ultimately gets fed into the model.\n",
    "\n",
    "We're using a language model pretrained on wikipedia text, then fine tuning it using our IMDB corpus. In the book they skip over where exactly the wikipedia model comes from, but looking into the library code for `language_model_learner`, it defaults to a pretrained model called `WT103_FWD`, which I assume is Wikipedia Text Forward (they have a backward one as well, wherein predictions are made by shifting the text in the opposite direction).\n",
    "\n",
    "The default loss function is cross entropy, since this is essentially a classification task (next word prediction probabilities for each of the words in the vocab). `Perplexity`is a metroc often used in NLP for language models, which is just the exponent of the loss function `torch.exp(cross_entropy)`.\n",
    "\n",
    "Like the vision learner, the language learner automatically calls `freeze` when using a pretrained model (the default), so this only trains the random embeddings of our IMDB corpus (tokens that exist in only the IMDB vocab but not the pretrained wikipedia vocab).\n",
    "\n",
    "Jeremy says we use `fit_one_cycle` instead of `fine_tune` because fine_tune doesn't save intermediate model results during training. Essentially, `fine_tune` automatically does what we have below - trains one epoch on the frozen model, unfreezes the model, then trains for more epochs. In order to save intermediate results, we split this out by calling fit_one_cycle for an epoch, save the model, unfreeze, then fit_one_cycle again for more epochs ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='105070592' class='' max='105067061' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [105070592/105067061 00:24&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = fai_text.language_model_learner(\n",
    "    dls_lm, fai_text.AWD_LSTM, drop_mult=0.3, \n",
    "    metrics=[fai_text.accuracy, fai_text.Perplexity()]).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.007815</td>\n",
       "      <td>3.900053</td>\n",
       "      <td>0.300463</td>\n",
       "      <td>49.405045</td>\n",
       "      <td>50:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.save('imdb_1epoch')\n",
    "learn = learn.load('imdb_1epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the first epoch has completed training on a completely frozen encoder (the body of the model, which is the whole model without the head), we unfreeze the whole thing and finish fine tuning the model, this time with a lower learning rate.\n",
    "\n",
    "Since my training was so slow, I'm just going to do 5 epochs. Should take most of the work day and be done. He had demoed with 10 epochs. I'll save after 5 epochs and could do 5 more epochs later if I wanted to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.742926</td>\n",
       "      <td>3.763016</td>\n",
       "      <td>0.315783</td>\n",
       "      <td>43.078175</td>\n",
       "      <td>50:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.661073</td>\n",
       "      <td>3.662944</td>\n",
       "      <td>0.327573</td>\n",
       "      <td>38.975933</td>\n",
       "      <td>47:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.546125</td>\n",
       "      <td>3.603450</td>\n",
       "      <td>0.334740</td>\n",
       "      <td>36.724709</td>\n",
       "      <td>1:02:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.440744</td>\n",
       "      <td>3.570328</td>\n",
       "      <td>0.339421</td>\n",
       "      <td>35.528236</td>\n",
       "      <td>1:14:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.359791</td>\n",
       "      <td>3.569659</td>\n",
       "      <td>0.339821</td>\n",
       "      <td>35.504482</td>\n",
       "      <td>54:49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(5, 2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model (this allows us to do text-generation,\n",
    "# which is effectively what a next-token prediction model is trained to do)\n",
    "learn.save('imdb_full')\n",
    "# And save just the encoder, which we use for the sentiment classification\n",
    "learn.save_encoder('imdb_encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "\n",
    "Now that we have a fine-tuned language model, we can use it to do next-token generation. To do this, we actually keep the head of our fine tuned model, because that's exactly what it was trained to do.\n",
    "\n",
    "Essentially, what we have here is an expert (and unfiltered) IMDB review generator. This is really cool because it is basically a tiny ChatGPT, fine tuned on the IMDB corpus for movie reviews. The key difference is that we don't have an RLHF (reinforcement learning, human feedback) step to follow up, so we can't just ask it questions and expect good answers like chatGPT is capable of. We need to carefully write prompts which will produce text that logically follows.\n",
    "\n",
    "Ultimately though, with good prompt engineering, we basically have a fine-tuned GPT model (though the original model is tiny, only trained on the wikipedia corpus).\n",
    "\n",
    "Note that temperature picks a random word based on the probabilities returned by the model for the vocab. The higher the temp, the more \"creative\" the model, in the sense that it has a higher chance to select words other than those with the highest probabilities. It effectively increases the randomness. For a low temp, the model produces \"cold\" rational answers in the sense that it almost always selects from just the highest probability word, so it effectively reduces the randomness.\n",
    "\n",
    "The formula is basically $tempPreds = preds / temp$, so a higher temp will increase the probabilities of all the lower probability words relative to the higest probability words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"I liked this movie because\"\n",
    "words_per_review = 40\n",
    "total_reviews = 2\n",
    "preds = [\n",
    "    learn.predict(prompt, words_per_review, temperature=0.75) \n",
    "    for _ in range(total_reviews)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i liked this movie because Johnny was a middle aged man and that he was a iv to Max ( he was a dark knight in the end ) . His relationship with King is very good because it was powerful',\n",
       " \"i liked this movie because of it , but to me was painful to watch . This movie is very bad . i saw the movie when i was in school , started to watch it , and was glad i did n't .\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'This xxunk movie sucked because of the high - schoolers , who were the ones who were allowed to be it . It is a very good story , but i hate it . Not so bad , i suppose . But'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(\n",
    "    \"This fucking movie sucked because\",\n",
    "    words_per_review,\n",
    "    temperature=0.75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'i really enjoyed this movie because it was so good . It was one of the best movies i have ever seen . The acting was very good , and the actors were great . And i was really surprised to see that'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(\n",
    "    \"I really enjoyed this movie because\",\n",
    "    words_per_review,\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we think through the implications of this, it is quite fascinating, because this is exactly how chatGPT was trained (again, other than RLHF).\n",
    "\n",
    "We did not explicitly teach the model english - we did not teach spelling, sentence structure, or grammar. And yet, the model has effectively learned to speak english, and with an IMDB review style dialect.\n",
    "\n",
    "The only thing we did is initialize a bunch of **random** parameters (an embedding for each word in the vocab), and by **self-supervised** learning, predicting the next word in a bunch of text (or, as with transformers, predicting words for blanks in text), these learned parameters have become  so effective that we've basically **produced a function that can write in english, and to a certain degree, even reason**.\n",
    "\n",
    "What we have now is a language model that can speak wikipedia-style English with an IMDB review dialect, but we can easily find models online like GPT-2 or some other ones that are open and much larger that can even translate between languages and write code. Then, we can fine tune them to any dialect we want (a corpus of documentation text, or even the writing style of any individual).\n",
    "\n",
    "Then, we can further fine tune these extremely effective models into very tightly controlled functions that do a specific thing really well, like classify IMDB reviews as positive or negative. For specific tasks like this, GPT-3 and up are probably absolute overkill anyway, and inference time would be dramatically faster on these smaller models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Finally, we can use our fine-tuned language model to create a fine-tuned classifier of IMDB reviews as positive or negative. For this, we do need labeled reviews (so this is not self-supervised).\n",
    "\n",
    "This makes the training process more similar to what we've done for our other training tasks - get a bunch of data with labels, and feed the data and the labels to the model to adjust its parameters.\n",
    "\n",
    "A couple of differences from a fastai perspective of how we train a language model vs. using a language model to train a classifier:\n",
    "* We do not pass `is_lm=True` to the TextBlock function.\n",
    "* We pass the vocab we created for the language model\n",
    "\n",
    "`is_lm=False` (the default) just tells fastai that we're using our own labels, rather than setting up a self-supervised next-word prediction dataloader.\n",
    "\n",
    "We also need to use the same vocab, otherwise the embeddings the model learned in the language model phase will make no sense to this classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_clas = fai_text.DataBlock(\n",
    "    blocks=(fai_text.TextBlock.from_folder(path, vocab=dls_lm.vocab),\n",
    "            fai_text.CategoryBlock),\n",
    "    get_y = fai_text.parent_label,\n",
    "    get_items=partial(fai_text.get_text_files, folders=['train', 'test']),\n",
    "    splitter=fai_text.GrandparentSplitter(valid_name='test')\n",
    ").dataloaders(path, path=path, bs=128, seq_len=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos * * attention xxmaj spoilers * * \\n\\n xxmaj first of all , let me say that xxmaj rob xxmaj roy is one of the best films of the 90 's . xxmaj it was an amazing achievement for all those involved , especially the acting of xxmaj liam xxmaj neeson , xxmaj jessica xxmaj lange , xxmaj john xxmaj hurt , xxmaj brian xxmaj cox , and xxmaj tim xxmaj roth . xxmaj michael xxmaj canton xxmaj jones painted a wonderful portrait of the honor and dishonor that men can represent in themselves . xxmaj but alas … \\n\\n it constantly , and unfairly gets compared to \" braveheart \" . xxmaj these are two entirely different films , probably only similar in the fact that they are both about xxmaj scots in historical xxmaj scotland . xxmaj yet , this comparison frequently bothers me because it seems</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos xxrep 3 * xxmaj warning - this review contains \" plot spoilers , \" though nothing could \" spoil \" this movie any more than it already is . xxmaj it really xxup is that bad . xxrep 3 * \\n\\n xxmaj before i begin , xxmaj i 'd like to let everyone know that this definitely is one of those so - incredibly - bad - that - you - fall - over - laughing movies . xxmaj if you 're in a lighthearted mood and need a very hearty laugh , this is the movie for you . xxmaj now without further ado , my review : \\n\\n xxmaj this movie was found in a bargain bin at wal - mart . xxmaj that should be the first clue as to how good of a movie it is . xxmaj secondly , it stars the lame action</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_clas.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding Documents\n",
    "\n",
    "The final step to fine tuning the classifier is making sure each document loaded into a batch is the same size. Since each document is a different length, we need to find a way to make them match up.\n",
    "\n",
    "With images, we did this with resizing (squishing), cropping, or padding. With text, we can do this with padding. We add a special token which the model ignores to shorter texts to make them the same size. For performance, we can collect the texts into batches of similar size (so there is less to pad) and use the longest text as the target length.\n",
    "\n",
    "This is done by the `TextBlock` automatically when `is_lm=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = fai_text.text_classifier_learner(dls_clas, fai_text.AWD_LSTM, drop_mult=0.5,\n",
    "                                        metrics=fai_text.accuracy).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load_encoder('imdb_encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning the Classifier\n",
    "\n",
    "Fastai indicates the best way to fine tune an NLP classifier is to use *gradual unfreezing* (in computer vision, we often unfreeze the model all at once), and we also use discriminitive learning rates (i.e., reduce the learning rate as you unfreeze, since the earlier layers are more fundamental and less related to the task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.368773</td>\n",
       "      <td>0.320571</td>\n",
       "      <td>0.867000</td>\n",
       "      <td>09:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 2e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Skipped the remaining training so I could experiment, could come back later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This unfreezes just the last 2 parameters - aka freezes all but the last 2\n",
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4), 1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('neg', tensor(0), tensor([0.9347, 0.0653]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(\"This was a bad movie, I did not like it, the story was poorly written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('neg', tensor(0), tensor([0.8711, 0.1289]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(\"\"\"The characterisations here are definitively subpar. No one looks or sounds like the people they portray, except for Laura Aikman, who looks quite a bit like and acts very much like her part. Therein lies the rub. Far too much effort to get that right that everything else fell by the wayside. The title is a misnomer, to say the least. It's not about Archie, it's not about Cary. It's about Dyan. It should have been called \"Dyan, Me, Me, Me and the 6 years I spent with that guy to whom I've served up a narcissistic manipulation pie with a light sprinkle of powdered truth\".\n",
    "\n",
    "He said it best, everything is a confrontation to her. And every confrontation in this lifetime movie of the week is a character assassination for every acquaintance she makes. Everything that goes wrong is always someone else's fault. He does drugs, he sold the dog, he's overbearing and controlling, his mother is a beach, his biz partner is a time stealer. Despite all her obvious flaws, the production makes everyone else out to be the bad guy and poor misunderstood her. Every scene is manicured and curated to paint everyone else in a bad light, and even when you think that maybe there's a bit of balance here, it quickly turns to self-victimizing pandering, expecting the audience to be too stupid to see it.\n",
    "\n",
    "An excellent study into the true character of a bitter ex-lover, but very little in the way of the person for whom we were duped into thinking it was about. It might think it's subtle, but it's not.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fastai Summary\n",
    "\n",
    "All in one place for ease of reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Hugging Face Transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
